---
title: "Clustering : High Cancer risk vs Low Cancer risk for counties in California"
author: "Ambareesh Jonnavittula"
date: "27/12/2021"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

## Contents

- Step 1 - Problem Definition
- Step 2 - Environmental Setup
- Step 3 - Data Acquisition
- Step 4 - Data Preparation
- Step 5 - Exploratory Data Analysis
- Step 6 - Data Preprocessing
- Step 7 - Clustering techniques
- Step 8 - Model Evaluation

## Step 1 - Problem Definition

Over the years, there have been various toxic pollutants which have caused Cancer among the people across the counties of California, USA.  

The objective of this task is to be able to cluster high risk counties vs. low risk counties, or identify if there could be a third tier (moderate).

_Note - In addition, this piece of code is reproducible, hence we could scale it to any state of the USA within the dataset_

## Step 2 - Environmental Setup

- dplyr : Data manipulations
- tidyverse : Data science tasks
- readxl : to Import the .xlsx file
- skimr : Statistical summary
- corrplot : Correlation matrix
- cluster : Clustering 
- factoextra : Clustering & Evaluation
- ggplot2 : Plotting graphs
- RColorBrewer : Colour palette

```{r include=FALSE}
# 2.1 - Install the required packages/libraries from R
# install.packages('dplyr')
# install.packages('tidyverse')
# install.packages('skimr')
# install.packages('cluster') 
# install.packages('factoextra') # install "factoextra" package
# install.packages("ggplot2") # for plotting graphs
# install.packages("corrplot") # for correlation matrix

# 2.2 - Load the libraries
library(dplyr) 
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(skimr)
library(corrplot)
library(readxl)  # To read .xlsx files
library(cluster) # Clustering
library(factoextra) # activate "factoextra" package

# 2.3 - Setup Working directory
fpath = "D:/University/ASDM/Developments/Task 3 - Clustering"
setwd(fpath)   # Set the WD
getwd()        # Validate the new WD
```

## Step 3 - Data Acquisition

- Import data from the file path using read_excel() function
- Rename columns for simplification
```{r echo=TRUE}
## Importing / Reading the data into "df_cea_raw" data frame 
df_cea_raw <- read_excel("ARM Dataset.xlsx", sheet = 1) 

# Inspect the raw data
head(df_cea_raw)
tail(df_cea_raw)
names(df_cea_raw)
summary(df_cea_raw)
str(df_cea_raw)
dim(df_cea_raw)

# Renaming the columns as part of simplification
names(df_cea_raw)[names(df_cea_raw) == 'State'] <-  'state'
names(df_cea_raw)[names(df_cea_raw) == 'County'] <-  'county'
names(df_cea_raw)[names(df_cea_raw) == 'Pollutant.Name'] <-  'pollutant'
names(df_cea_raw)[names(df_cea_raw) == 'Point..includes.railyards..Cancer.Risk..per.million.'] <-  'railyards_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'Airport.Cancer.Risk..per.million.'] <-  'airport_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'OR.Lightduty..includes.refueling..Cancer.Risk..per.million.'] <-  'lightduty_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'OR.Heavyduty.Cancer.Risk..per.million.'] <-  'heavyduty_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'NR..no.airports..CMV..locomotives..Cancer.Risk..per.million.'] <-  'cmv_loco_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'NP.10m.ReleaseHeight.Cancer.Risk..per.million.'] <-  'np_10m_releaseheight_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'NP.Low.ReleaseHeight.Cancer.Risk..per.million.'] <-  'np_low_releaseheight_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'ResidentialWoodCombustion..RWC..Cancer.Risk..per.million.'] <-  'rwc_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'NR.CommercialMarineVessel..CMV..Cancer.Risk..per.million.'] <-  'cmv_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'Biogenics.Cancer.Risk..per.million.'] <-  'biogenics_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'Fires..ag..prescribed..and.wild..Cancer.Risk..per.million.'] <-  'fires_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'Secondary.Cancer.Risk..per.million.'] <-  'secondary_crpm'
names(df_cea_raw)[names(df_cea_raw) == 'Total.Cancer.Risk..per.million.'] <-  'total_crpm'
```

## Step 4 - Data Preparation

1. Subset to a single state
2. Aggregate cancer risk data per category using averages per county
```{r echo=TRUE}
# 4.1 Subset to state CA
df_cea <- select(filter(df_cea_raw, state == "CA")
                 ,c(state, county, #pollutant,
                    railyards_crpm,
                    airport_crpm,
                    lightduty_crpm,
                    heavyduty_crpm,
                    cmv_loco_crpm,
                    np_10m_releaseheight_crpm,
                    np_low_releaseheight_crpm,
                    rwc_crpm,
                    cmv_crpm,
                    biogenics_crpm,
                    fires_crpm,
                    secondary_crpm,
                    total_crpm  ))

# 4.2 Aggregate data using averages
df_cea_agg <- df_cea %>%
              group_by(state, county) %>%
              summarise_at(vars(c(1:13)), list(avg = mean))

# 4.3 Drop the raw dataframe
remove(df_cea_raw)
```

3. Inspect the subset 
```{r echo=TRUE}
# 4.3 Inspect the subset
# Describing the data subset 
head(df_cea)
tail(df_cea)
names(df_cea)
str(df_cea)
summary(df_cea)

# Check the number of Rows & Columns in the data subset
nrow(df_cea) 
ncol(df_cea) 
dim(df_cea)   

skim(df_cea)

# Describing the data subset 
head(df_cea_agg)
tail(df_cea_agg)
names(df_cea_agg)
str(df_cea_agg)
summary(df_cea_agg)

# Check the number of Rows & Columns in the data subset
nrow(df_cea_agg) 
ncol(df_cea_agg) 
dim(df_cea_agg)   

skim(df_cea_agg)
```

3. Custom function to normalize the data
```{r echo=TRUE}
#normalise function
normalise <- function(df)
{
  return(((df- min(df)) /(max(df)-min(df))*(1-0))+0)
}
```

## Step 5 - Exploratory Data Analysis

1. Correlation plots with Pearson correlation coefficients
```{r echo=TRUE}
# Pairs plot - Correlation plot
df_cea_agg$state <- as.factor(df_cea_agg$state)
df_cea_agg$county <- as.factor(df_cea_agg$county)
pairs(df_cea_agg)

rownames(df_cea_agg) <- df_cea_agg$county

corrmatrix <- cor(df_cea_agg[,3:15])
corrplot(corrmatrix, method = 'number')
```

From above outcomes, we can deduce that the average total CRPM has strong correlations with averages of light duty crpm, heavy duty crpm, cmv loco crpm & np_low_release height crpm.

2. Strong Correlations (based on EDA)
```{r echo=TRUE}
# Total CRPM avg vs. Light duty CRPM avg  
plot(total_crpm_avg ~ lightduty_crpm_avg, data = df_cea_agg)
with(df_cea_agg,text(total_crpm_avg ~ lightduty_crpm_avg, labels= county,pos=4,cex=.6))

# Total CRPM avg vs. Heavy duty CRPM avg  
plot(total_crpm_avg ~ heavyduty_crpm_avg, data = df_cea_agg)
with(df_cea_agg,text(total_crpm_avg ~ heavyduty_crpm_avg, labels= county,pos=4,cex=.6))

# Total CRPM avg vs. CMV_Loco_CRPM_avg 
plot(total_crpm_avg ~ cmv_loco_crpm_avg, data = df_cea_agg)
with(df_cea_agg,text(total_crpm_avg ~ cmv_loco_crpm_avg, labels= county,pos=4,cex=.6))

# Total CRPM avg vs. np_low_release_Height_crpm_avg
plot(total_crpm_avg ~ np_low_releaseheight_crpm_avg, data = df_cea_agg)
with(df_cea_agg,text(total_crpm_avg ~ np_low_releaseheight_crpm_avg, labels= county,pos=4,cex=.6))
```

## Step 6 - Data Preprocessing

1. Normalize all the continuous CRPM variables.
2. Inspect the normalized dataset.
```{r echo=TRUE}
# 6.1.1 Separate the county
df_cea_agg$county <- as.character(df_cea_agg$county)
county <- df_cea_agg[,2]
county

# 6.1.2 Normalise the dataset using the custom created function
df_cea_agg_n <- as.data.frame(lapply(df_cea_agg[,3:15] ,normalise))

# 6.1.3 Merge with County, apply rownames & sort by County
df_cea_agg_n <- cbind(county, df_cea_agg_n)
rownames(df_cea_agg_n)<-df_cea_agg_n$county
df_cea_agg_n <- df_cea_agg_n[order(county),]

# 6.2 Inspect the normalized dataset
dim(df_cea_agg_n)
str(df_cea_agg_n)
names(df_cea_agg_n)
head(df_cea_agg_n)
tail(df_cea_agg_n)
```

3. Create and analyze the distance between spatial points using the Euclidean method, and create a distance matrix
```{r echo=TRUE}
# 6.3.1 Choosing Euclidean distance method and creating distance matrix
distance <- dist(df_cea_agg_n,method = "euclidean")
print(distance,digits=3)

get_dist(distance)
fviz_dist(distance, order = TRUE, gradient = list(low = "red", mid = "white", high = "green"))

```

4. Saving the cleaned dataset as a .csv (for SAS)
```{r echo=TRUE}
# 4.1 - Write to a .csv file
write.csv(df_cea_agg_n, "Average_CancerRisk_per_County.csv", row.names=TRUE)

```

## Step 7 - Clustering techniques
### 7.1 - Hierarchical Clustering
```{r echo=TRUE}
# 7.1.1 - Cluster Dendrogram
df_cea_agg.hclust <- hclust(distance)
df_cea_agg.hclust

plot(df_cea_agg.hclust)
plot(df_cea_agg.hclust,hang=-1)

rect.hclust(df_cea_agg.hclust, 4)

# 7.1.2 Hierarchical clustering using average linkage
hclust.average <- hclust(distance, method = "average")
plot(hclust.average)
rect.hclust(hclust.average, 4)

# 7.1.3 Hierarchical clustering using single linkage
hclust.single <- hclust(distance, method = "single")
plot(hclust.single,hang = -1)
rect.hclust(hclust.single, 4)

# 7.1.4 Hierarchical clustering using centroid linkage
hclust.centroid <- hclust(distance, method = "centroid")
plot(hclust.centroid,hang = -1)
rect.hclust(hclust.centroid, 4)

# 7.1.5 Hierarchical clustering using complete linkage
hclust.complete <- hclust(distance, method = "complete")
plot(hclust.complete,hang = -1)
rect.hclust(hclust.complete, 4)

# 7.1.6 Members of dendrogram
member.centroid <- cutree(hclust.centroid,4) 
member.centroid
member.complete <- cutree(hclust.complete,4)
member.complete

table(member.centroid,member.complete)
```

### 7.2 K-Means Clustering
#### 1. Identify the optimal number of clusters
```{r echo=TRUE}
# 7.2.1 - Scale the continuous data
scaled_df <- scale(df_cea_agg_n[,2:14])

# 7.2.2 - Calculate the Optimal number of clusters using factoextra package
fviz_nbclust(scaled_df, kmeans, method='silhouette')
```

#### 2. Plot the graphs to observe the distinction/overlap between the clusters.
```{r echo=TRUE}
# 7.2.3 - Apply the kmeans() algorithm based on the optimal no. of clusters
kclus <- kmeans(scaled_df, 2)$cluster

# 7.2.4 - Plot the graphs (without data points)
clusplot(scaled_df, kclus, main = "High Cancer risk vs Low Cancer risk counties in California, USA")

# 7.2.5 - Plot the graphs (with data points)
clusplot(scaled_df, kclus, color = TRUE, shade = TRUE, cex=0.6, labels = 2, lines = 0, main = "High Cancer risk vs Low Cancer risk counties in California, USA")
```

## Step 8 - Model Evaluation
```{r echo=TRUE}
# Silhoutte Score
silhouette_score <- function(k){
  km <- kmeans(scaled_df, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(scaled_df))
  mean(ss[, 3])
}
k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, main = 'Average Silhoutte Scores', xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

```

